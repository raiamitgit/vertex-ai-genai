{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afBCLvNQ6ZKQ"
      },
      "source": [
        "# Document AI Layout Parser: Extract Tables from PDF Document\n",
        "\n",
        "This notebook demonstrating how to extract tables from PDF documents using Google Cloud Document AI's Layout Parser processor. The extracted tables are converted into Pandas DataFrames for easy analysis and manipulation.\n",
        "\n",
        "This approach is useful for structuring information found in visually complex documents. It can alos be a valuable component in Retrieval-Augmented Generation (RAG) systems by providing structured data alongside textual content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BxL9NL0wt0b"
      },
      "source": [
        "## Overview\n",
        "\n",
        "It performs the following steps:\n",
        "\n",
        "1.  **Setup & Authentication:** Installs necessary libraries and authenticates with Google.\n",
        "2.  **Configuration:** Sets required Google Cloud project details, Document AI processor information, and the location of the input PDF file in Google Cloud Storage (GCS).\n",
        "3.  **Document Processing:** Calls the Document AI API using the specified Layout Parser processor to analyze the document's structure, including text, paragraphs, and tables. Chunking options can also be configured.\n",
        "4.  **Table Identification:** Recursively parses the Layout Parser response  to find elements identified as tables (`table_block`).\n",
        "5.  **Table Conversion:** Extracts text from table cells and reconstructs each identified table into a Pandas DataFrame.\n",
        "6.  **Output:** Prints the extracted tables (as DataFrames) along with their corresponding page number(s) in the original document.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqUwYnXttywB"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "Before running this notebook, ensure you have the following:\n",
        "\n",
        "1.  **Google Cloud Project:** A Google Cloud Platform project with billing enabled.\n",
        "2.  **APIs Enabled:** The Document AI API must be enabled in your GCP project.\n",
        "3.  **Document AI Processor:**\n",
        "    *   A Document AI processor created. For this notebook, the **Layout Parser** processor type is expected.\n",
        "    *   Note down the **Processor ID** and the **Location** (region, e.g., `us`, `eu`) where it was created.\n",
        "    *   You can use a specific processor version (e.g., `pretrained-layout-v1.0-2022-11-10`) or a stable alias like `rc` (release candidate) or `stable`. Refer to [Managing Processor Versions](https://cloud.google.com/document-ai/docs/manage-processor-versions).\n",
        "4.  **Google Cloud Storage (GCS):**\n",
        "    *   A GCS bucket within your project.\n",
        "    *   The PDF document you want to process must be uploaded to this bucket. Note down the **GCS URI** (e.g., `gs://your-bucket-name/path/to/your-document.pdf`).\n",
        "5.  **Python Environment:** Python 3.7+ installed.\n",
        "6.  **Required Libraries:** Install the necessary Python packages:\n",
        "    ```bash\n",
        "    pip install -U google-cloud-documentai pandas google-auth\n",
        "    ```\n",
        "7.  **Authentication:** You need to be authenticated to Google Cloud. How you do this depends on your environment:\n",
        "    *   **Google Colab:** The notebook uses `google.colab.auth.authenticate_user()`.\n",
        "    *   **Local Development/VM/Cloud Shell:** Use the Google Cloud SDK (`gcloud`):\n",
        "        ```bash\n",
        "        gcloud auth application-default login\n",
        "        ```\n",
        "    *   **Service Account:** Set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to the path of your service account key file.\n",
        "8.  **Permissions:** Ensure the authenticated principal (user or service account) has sufficient IAM permissions, typically including:\n",
        "    *   `Document AI User` (roles/documentai.user)\n",
        "    *   `Storage Object Viewer` (roles/storage.objectViewer) on the input GCS file/bucket."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-BYuuDC6ZAS"
      },
      "source": [
        "## Basic Setup\n",
        "Install dependencies and authenticate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIuv2L3TTGlH"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q google\n",
        "!pip install -U -q google-cloud-documentai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7s5U3Bsfh_X"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import pandas as pd\n",
        "from typing import Optional, Sequence\n",
        "\n",
        "from google.cloud import documentai\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from google.api_core.exceptions import GoogleAPICallError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peJA3hmzQF4w"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYfLKD3iKRDO"
      },
      "outputs": [],
      "source": [
        "# Configure GCP environment\n",
        "PROJECT_ID = \"my-gcp-project-id\"\n",
        "LOCATION = \"us\"\n",
        "PROCESSOR_ID = \"processor_id\"\n",
        "PROCESSOR_VERSION = \"rc\" # Refer to https://cloud.google.com/document-ai/docs/manage-processor-versions for more information\n",
        "GCS_FILE_PATH = \"gs://my-bucket/my-folder/my-file.pdf\"\n",
        "mime_type = \"application/pdf\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zilWTZjR8sX"
      },
      "source": [
        "## DocAI Layout Parser\n",
        "For extracting structured information from unstructred documents\n",
        "Excellent for building RAG applications and mitigating hallucinations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdI4x9lqSBg6"
      },
      "outputs": [],
      "source": [
        "def process_document_for_layout(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    processor_id: str,\n",
        "    processor_version: str,\n",
        "    gcs_uri: str,\n",
        "    mime_type: str,\n",
        ") -> documentai.Document:\n",
        "    \"\"\"\n",
        "    Processes a document using a Document AI processor with specific options\n",
        "    configured for layout analysis, including chunking.\n",
        "\n",
        "    Args:\n",
        "        project_id: The Google Cloud project ID.\n",
        "        location: The location (region) of the Document AI processor (e.g., \"us\", \"eu\").\n",
        "        processor_id: The ID of the Document AI processor.\n",
        "        processor_version: The specific version of the processor to use.\n",
        "        gcs_uri: The Google Cloud Storage URI of the document to process.\n",
        "        mime_type: The MIME type of the document (e.g., \"application/pdf\", \"image/jpeg\").\n",
        "\n",
        "    Returns:\n",
        "        The processed documentai.Document object, containing layout information\n",
        "        and potentially chunked data based on the specified options.\n",
        "    \"\"\"\n",
        "    # Define specialized processing options for layout extraction and chunking.\n",
        "    # Chunking helps break down the document into smaller, semantically related pieces.\n",
        "    process_options = documentai.ProcessOptions(\n",
        "        layout_config=documentai.ProcessOptions.LayoutConfig(\n",
        "            chunking_config=documentai.ProcessOptions.LayoutConfig.ChunkingConfig(\n",
        "                chunk_size=1000,  # Target size for each chunk (in characters).\n",
        "                include_ancestor_headings=True, # Include relevant headings with chunks.\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Call the generic processing function with the layout-specific options.\n",
        "    document = process_document(\n",
        "        project_id=project_id,\n",
        "        location=location,\n",
        "        processor_id=processor_id,\n",
        "        processor_version=processor_version,\n",
        "        gcs_uri=gcs_uri,\n",
        "        mime_type=mime_type,\n",
        "        process_options=process_options,\n",
        "    )\n",
        "\n",
        "    # The returned 'document' object contains the results, including:\n",
        "    # - document.document_layout.blocks: For layout block information.\n",
        "    # - document.chunked_document.chunks: If chunking was enabled and successful.\n",
        "    # The caller can now access these attributes as needed.\n",
        "    # Example:\n",
        "    # print(\"Document Layout Blocks Found:\")\n",
        "    # for block in document.document_layout.blocks:\n",
        "    #     print(f\"  Block ID: {block.block_id}, Confidence: {block.confidence:.2f}\")\n",
        "    #\n",
        "    # print(\"\\nDocument Chunks Found:\")\n",
        "    # for chunk in document.chunked_document.chunks:\n",
        "    #     print(f\"  Chunk ID: {chunk.chunk_id}, Content Snippet: '{chunk.content[:50]}...'\")\n",
        "\n",
        "    return document\n",
        "\n",
        "\n",
        "def process_document(\n",
        "    project_id: str,\n",
        "    location: str,\n",
        "    processor_id: str,\n",
        "    processor_version: str,\n",
        "    gcs_uri: str,\n",
        "    mime_type: str,\n",
        "    process_options: Optional[documentai.ProcessOptions] = None,\n",
        ") -> documentai.Document:\n",
        "    \"\"\"\n",
        "    Processes a document stored in Google Cloud Storage using a specified\n",
        "    Document AI processor version.\n",
        "\n",
        "    Args:\n",
        "        project_id: The Google Cloud project ID.\n",
        "        location: The location (region) of the Document AI processor (e.g., \"us\", \"eu\").\n",
        "        processor_id: The ID of the Document AI processor.\n",
        "        processor_version: The specific version of the processor to use.\n",
        "        gcs_uri: The Google Cloud Storage URI of the document to process.\n",
        "        mime_type: The MIME type of the document (e.g., \"application/pdf\", \"image/jpeg\").\n",
        "        process_options: Optional. Specific processing configurations (e.g., OCR versions,\n",
        "                         layout options).\n",
        "\n",
        "    Returns:\n",
        "        The processed documentai.Document object returned by the API.\n",
        "\n",
        "    Raises:\n",
        "        google.api_core.exceptions.GoogleAPICallError: If the API call fails.\n",
        "        # Other potential exceptions like PermissionError, FileNotFoundError (if GCS URI is invalid)\n",
        "    \"\"\"\n",
        "    # You must set the `api_endpoint` if your processor is not in the default \"us\" location.\n",
        "    client_options = ClientOptions(\n",
        "        api_endpoint=f\"{location}-documentai.googleapis.com\"\n",
        "    )\n",
        "\n",
        "    # Initialize the Document AI client.\n",
        "    client = documentai.DocumentProcessorServiceClient(client_options=client_options)\n",
        "\n",
        "    # Construct the full resource name for the processor version.\n",
        "    # Example: projects/YOUR_PROJECT_ID/locations/us/processors/YOUR_PROCESSOR_ID/processorVersions/YOUR_PROCESSOR_VERSION\n",
        "    # Ensure the processor and version exist and are deployed/enabled in the Cloud Console.\n",
        "    name = client.processor_version_path(\n",
        "        project_id, location, processor_id, processor_version\n",
        "    )\n",
        "\n",
        "    # Specify the document source from Google Cloud Storage.\n",
        "    gcs_document = documentai.GcsDocument(gcs_uri=gcs_uri, mime_type=mime_type)\n",
        "\n",
        "    # Configure the process request.\n",
        "    request = documentai.ProcessRequest(\n",
        "        name=name,\n",
        "        gcs_document=gcs_document,\n",
        "        process_options=process_options, # Pass any specific options provided.\n",
        "        # You can also skip human review if not needed for straight-through processing:\n",
        "        # skip_human_review=True\n",
        "    )\n",
        "\n",
        "    # Make the synchronous API call to process the document.\n",
        "    result = client.process_document(request=request)\n",
        "\n",
        "    # Return the main Document object from the API response.\n",
        "    return result.document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvoaZmEorXvC"
      },
      "outputs": [],
      "source": [
        "pdf_layout = process_document_for_layout(\n",
        "    PROJECT_ID,\n",
        "    LOCATION,\n",
        "    PROCESSOR_ID,\n",
        "    PROCESSOR_VERSION,\n",
        "    GCS_FILE_PATH,\n",
        "    mime_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDIPIkGQ28aj",
        "outputId": "07bfacf2-8e75-4b1f-93f9-55e4cd493cf6"
      },
      "outputs": [],
      "source": [
        "pdf_layout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDFEBQQ9zEb6"
      },
      "source": [
        "### Reconstruct markdown table from json response\n",
        "Extract and organize the table details from the layout parser response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BsE6GyMxwer2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import List, Tuple, Any, Optional\n",
        "\n",
        "def extract_text_from_cell(cell: Any) -> str:\n",
        "    \"\"\"\n",
        "    Extracts and concatenates text from text blocks within a cell structure.\n",
        "\n",
        "    Uses getattr for safe access on potentially dict or proto message objects.\n",
        "\n",
        "    Args:\n",
        "        cell: A cell structure, potentially containing 'blocks' or a 'text_block'.\n",
        "\n",
        "    Returns:\n",
        "        The concatenated text content, stripped and joined by spaces,\n",
        "        or an empty string if no text is found.\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    # Primary path: Look for blocks within the cell\n",
        "    blocks = getattr(cell, 'blocks', None)\n",
        "    if blocks:\n",
        "        try:\n",
        "            for block in blocks:\n",
        "                text_block = getattr(block, 'text_block', None)\n",
        "                text_content = getattr(text_block, 'text', None)\n",
        "                if text_content:\n",
        "                    texts.append(str(text_content).strip())\n",
        "        except TypeError:\n",
        "            pass\n",
        "\n",
        "    # Fallback path: Look for a direct text_block under the cell if no text found yet\n",
        "    if not texts:\n",
        "        text_block = getattr(cell, 'text_block', None)\n",
        "        text_content = getattr(text_block, 'text', None)\n",
        "        if text_content:\n",
        "            texts.append(str(text_content).strip())\n",
        "\n",
        "    return \" \".join(filter(None, texts)) # Join non-empty strings\n",
        "\n",
        "def table_to_dataframe(table_block: Any) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converts a table_block structure into a Pandas DataFrame.\n",
        "\n",
        "    Uses getattr for safe access and list comprehensions for clarity.\n",
        "\n",
        "    Args:\n",
        "        table_block: The table_block element (dict or proto message).\n",
        "\n",
        "    Returns:\n",
        "        A Pandas DataFrame representing the table, or an empty DataFrame\n",
        "        if the table is invalid, empty, or processing fails.\n",
        "    \"\"\"\n",
        "    header_texts = []\n",
        "    data_rows_list = []\n",
        "    num_columns = 0\n",
        "\n",
        "    rows = getattr(table_block, 'body_rows', [])\n",
        "    if not rows:\n",
        "        return pd.DataFrame() # No body rows found\n",
        "\n",
        "    try:\n",
        "        # --- Header Extraction ---\n",
        "        first_row_cells = getattr(rows[0], 'cells', [])\n",
        "        if not first_row_cells:\n",
        "            return pd.DataFrame() # First row has no cells\n",
        "        header_texts = [extract_text_from_cell(cell) for cell in first_row_cells]\n",
        "        num_columns = len(header_texts)\n",
        "\n",
        "        # --- Data Row Extraction ---\n",
        "        for row_data in rows[1:]:\n",
        "            data_cells = getattr(row_data, 'cells', [])\n",
        "            # Extract text, handling potential iteration errors softly within extract_text_from_cell\n",
        "            row_texts = [extract_text_from_cell(cell) for cell in data_cells]\n",
        "\n",
        "            # Pad or truncate row to match header column count\n",
        "            if len(row_texts) < num_columns:\n",
        "                row_texts.extend([\"\"] * (num_columns - len(row_texts)))\n",
        "            elif len(row_texts) > num_columns:\n",
        "                row_texts = row_texts[:num_columns]\n",
        "\n",
        "            data_rows_list.append(row_texts)\n",
        "\n",
        "        # --- DataFrame Creation ---\n",
        "        return pd.DataFrame(data_rows_list, columns=header_texts)\n",
        "\n",
        "    except (IndexError, TypeError, AttributeError, Exception) as e:\n",
        "        # Catch potential errors during row/cell access or DataFrame creation\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def find_and_extract_tables(data: Any) -> List[Tuple[pd.DataFrame, Optional[Any]]]:\n",
        "    \"\"\"\n",
        "    Recursively finds 'table_block' attributes within a nested data structure\n",
        "    (list, dict, or proto message) and returns tables as DataFrames\n",
        "    along with any associated 'page_span' found at the same level.\n",
        "\n",
        "    Args:\n",
        "        data: The data structure or a sub-part of it during recursion.\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples, where each tuple is (pd.DataFrame, page_span_object or None).\n",
        "    \"\"\"\n",
        "    tables_with_pages: List[Tuple[pd.DataFrame, Optional[Any]]] = []\n",
        "\n",
        "    # 1. Handle list-like structures (includes lists, tuples, proto repeated fields)\n",
        "    #    Check for __iter__ but exclude strings/bytes.\n",
        "    is_iterable = hasattr(data, '__iter__') and not isinstance(data, (str, bytes))\n",
        "    if is_iterable:\n",
        "        try:\n",
        "            for item in data:\n",
        "                tables_with_pages.extend(find_and_extract_tables(item))\n",
        "        except TypeError:\n",
        "            # Handle cases where iteration might fail unexpectedly\n",
        "             pass # Cannot iterate this item, continue searching elsewhere\n",
        "\n",
        "    # 2. Handle dict-like structures or objects (includes dicts, proto messages)\n",
        "    #    Check if it's not obviously iterable OR if it has dict-like features\n",
        "    elif not is_iterable or hasattr(data, 'ListFields') or isinstance(data, dict):\n",
        "        # Attempt to get page_span from the current object/dict level\n",
        "        page_span_info = getattr(data, 'page_span', None)\n",
        "\n",
        "        # A. Check directly for a valid 'table_block' at this level\n",
        "        table_block = getattr(data, 'table_block', None)\n",
        "        # Pre-check if the table_block seems minimally valid (has body_rows)\n",
        "        if table_block and getattr(table_block, 'body_rows', None):\n",
        "            df = table_to_dataframe(table_block)\n",
        "            if not df.empty:\n",
        "                tables_with_pages.append((df, page_span_info))\n",
        "                # Do not recurse further into this table_block's own potential 'blocks'\n",
        "                # as we've already processed it as a table.\n",
        "\n",
        "        # B. If it wasn't primarily a table block itself, *then* recurse into children\n",
        "        else:\n",
        "             # Recurse into 'blocks' attribute if present\n",
        "             blocks = getattr(data, 'blocks', None)\n",
        "             if blocks:\n",
        "                 # Use helper function to avoid code duplication for recursion\n",
        "                 tables_with_pages.extend(find_and_extract_tables(blocks))\n",
        "\n",
        "             # Recurse into 'text_block.blocks' if present\n",
        "             text_block = getattr(data, 'text_block', None)\n",
        "             text_block_blocks = getattr(text_block, 'blocks', None)\n",
        "             if text_block_blocks:\n",
        "                 tables_with_pages.extend(find_and_extract_tables(text_block_blocks))\n",
        "\n",
        "             # Note: Original Step 4 (recursion into cells) is intentionally omitted as per\n",
        "             # the comment in the original code (\"REVISED: Removed deep recursion...\")\n",
        "\n",
        "    # If data is neither iterable nor object-like (e.g., primitive types), recursion stops here.\n",
        "    return tables_with_pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNhWID9y5c8o",
        "outputId": "6d1e5bd0-5bbb-432f-9c12-63dc9fb8b19e"
      },
      "outputs": [],
      "source": [
        "# Extract and print tables\n",
        "input_data = pdf_layout\n",
        "\n",
        "all_tables_with_pages = []\n",
        "# Check if the expected top-level attributes exist using hasattr\n",
        "if hasattr(input_data, 'document_layout') and input_data.document_layout is not None and \\\n",
        "   hasattr(input_data.document_layout, 'blocks'):\n",
        "    try:\n",
        "        # Start searching from the blocks collection\n",
        "        all_tables_with_pages = find_and_extract_tables(input_data.document_layout.blocks)\n",
        "        # Deduplication based on DataFrame content is complex and omitted.\n",
        "    except Exception as e:\n",
        "         print(f\"An error occurred during table processing: {e}\")\n",
        "else:\n",
        "    print(\"Error: Input data does not match expected structure (missing 'document_layout' or 'document_layout.blocks').\")\n",
        "\n",
        "\n",
        "# Print the results\n",
        "if all_tables_with_pages:\n",
        "    print(f\"Found {len(all_tables_with_pages)} table(s).\\n\")\n",
        "    # Optional: Set pandas display options for potentially better formatting\n",
        "    # pd.set_option('display.max_rows', None) # Show all rows\n",
        "    # pd.set_option('display.max_columns', None) # Show all columns\n",
        "    # pd.set_option('display.width', 1000) # Adjust display width\n",
        "    # pd.set_option('display.colheader_justify', 'left') # Left-align headers\n",
        "\n",
        "    for i, (df, page_span) in enumerate(all_tables_with_pages):\n",
        "        # Format page number information\n",
        "        page_str = \"Page Unknown\"\n",
        "        if page_span and hasattr(page_span, 'page_start') and hasattr(page_span, 'page_end'):\n",
        "             start = page_span.page_start\n",
        "             end = page_span.page_end\n",
        "             if start == end:\n",
        "                 page_str = f\"Page {start}\"\n",
        "             elif start is not None and end is not None: # Check both exist\n",
        "                 page_str = f\"Pages {start}-{end}\"\n",
        "             elif start is not None: # Only start exists\n",
        "                 page_str = f\"Page {start} (end unknown)\"\n",
        "             elif end is not None: # Only end exists (less likely)\n",
        "                 page_str = f\"Page unknown-{end}\"\n",
        "\n",
        "        print(f\"--- Table {i+1} ({page_str}) ---\") # Include page info in the header\n",
        "        # Use to_string() for a clean, aligned text representation without index\n",
        "        print(df.to_string(index=False, justify='left', na_rep='')) # na_rep='' prints empty string for missing values\n",
        "        print(\"\\n\" + \"=\"*40 + \"\\n\") # Separator\n",
        "else:\n",
        "    # Check if an error message was already printed\n",
        "    error_printed = False\n",
        "    if not (hasattr(input_data, 'document_layout') and input_data.document_layout is not None and hasattr(input_data.document_layout, 'blocks')):\n",
        "        error_printed = True\n",
        "    # If the structure was valid but no tables were found, print the message\n",
        "    if not error_printed and not all_tables_with_pages: # Ensure no tables were found after valid structure check\n",
        "         print(\"No tables found in the input data.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMeLc2qlkvUx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
